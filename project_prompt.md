Your task is to write a tool to retrieve information from multiple websites in a batch operation. Read the docs in the `docs` folder for this task. The entry point for the program should be `main.py`, but you can create as many `py` files as is convenient, to nicely organize the code.

# Session ID
When the program starts, it needs to create a sessionid, which will simply be the unix time at startup, in seconds.

# LLM access
Look at the file `~/monorepo/common/ai.py`, we only need to be able to call `openai()` and `gemini_fast()`. Copy over the needed code paths in their entirety to the current directory.

The only three models you should use for this project are "gpt-5.2", "gemini-3-flash-preview", and openai "computer-use-preview".

# Logging of information
To perform the web retrievals efficiently, the tool will log all previous attempts to perform a retrieval, using this history to work smarter. For every enabled query, it should create a directory (example: "twitter.com 0" should lead to the directory "twitter_com_0" being used.) In that directory it should create a file "{sessionid}.plan" that contains a plan file for the current sessions's attempt at answering that query. Once the query is attempted (successfully or unsuccessfully) it will also create a subdirectory inside that directory named "{sessionid}_artifacts", where we will store artifacts that we received from the website, as part of the query retrieval for that query.

# Creating the Plan file
If no previous plan files exist for a query, we need to come up with an initial plan: Use `gemini_fast` and give it the query info, and have it come up with a starting plan that it thinks will work and that is appropriate for the site and query. It will need detailed knowledge of the plan file format and should output a correct plan in that format. Here are some of the sort of things it should figure out:
- For sites/queries that typically require a login it should require a login in the plan
- It shouldn't require simulated actions like click etc, since those require knowledge about the website that is unavailable to start, it first will have to explore the website
- If the website will likely need a text entry operation to complete the query, it should add `enable_text_entry`. For example, looking up the user's stock portfolio should not require text entry after the user is logged in. Looking up the price of a specific stock ticker WOULD require text entry of the stock ticker.
- It needs to break the query into the "exploratory" aspect of the query (i.e. describing what parts of the website need to be found) and then the "answering" aspect of the query, giving the precise data points that will need to be answered to successfully complete the query

# Creating an updated plan, if previous plans exist already
If the site/query has already been executed before, take the last 10 plans and results of those plans (only in that query directory, so in the `site_number` directory) and give those to `gemini_fast` and ask it to formulate a new plan for the next run, based on the previous runs. (If a plan is missing a result, don't include that plan). Obviously, if previous plans FAILed, it should try to avoid failing again. If previous runs were successful, it should try to make the new plan less burdensome if possible (require less time to run or require less human interaction).

# Session state
In the `websites.md` file there may be multiple queries for the same website. However, we can always reuse the session information (and hence may be able to avoid multiple human logins to that site). Store the session information for a given site in a root level "session" directory (example: "twitter_com_session/").

# Executing the Plan
Next, the program should parse and execute the plan. Look at the working legacy code in `main_legacy.py` and have the plan commands perform those actions: But try to simplify the bloated legacy code, have the new code only do the simplest, minimal work to perform each action.

Any command that interacts with the website (whether direct interaction instructions like "click" or "type", or interations created by the exploration, should place artifacts in the "{sessionid}_artifacts" folder. Have a sequencer that iterates with each interaction and capture both images and html, so there should be files like `screenshot_0.png` and `page_0.html` in that folder.

If an "explore website" command is used, it should additionally create a `exploration_steps.json` file in the artifact folder, once it has finished running. That file should list all the individual interactions generated by the computer use api. This file should be included in gemini_fast query for creating updated plans: If the query completed successfully, when the llm updates the plan, it may want to substitute the concrete interactions instead of the exploration (so that we get better performance by avoiding the llm calls involved in exploration)

For the "answer query" commands, use openai "gpt-5.2". If using the text command `answer_query_text`, first preprocess the html with `html_preprocessor.py` before sending to the llm (but save the unprocessed html in the artifact).

Once you have completed the plan, generate the results file in the query directory (e.g. `twitter_com_0/{sessionid}_results.md`)

# Parallel processing
By default, the program should process all the sites&queries in parallel (max 8 threads). However, allow an optional command line argument "--serial" that instead processes them all serially, in order.
Whenever one of the retrievals completes (whether in parallel or serial) print the "pertinent results" to stdout. If the retrieval failed, just write a single line indicating that it failed, along with the filename of the results file for the retrieval (so that the user can look at that file for the full error info)
If multiple sites require human login, serialize the login prompts for the user, to avoid stdin conflicts.
